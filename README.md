# Udacity Data Engineering Nanodegree 
## Project 4: Data Lake with Apache Spark

### Background
#### Introduction
A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.

You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

#### Project Description
In this project, you'll apply what you've learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. To complete the project, you will need to load data from S3, process the data into analytics tables using Spark, and load them back into S3. You'll deploy this Spark process on a cluster using AWS.

### Project Datasets
2 datasets - Song Data and Log Data

#### Song Dataset
Features:
- Subset of real data from the Million Song Dataset
- JSON format
- Contains metadata about a song and the artist of that song.
- Partitioned by the first three letters of each song's track ID
- Sample filepaths for 2 files in the dataset:
`song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json`
- Sample of single song file (TRAABJL12903CDCF1A.json):
`{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}`

#### Log Dataset
Features:
- Log files generated by event simulator based on songs in the dataset above
- Partitioned by year and month
- Sample filepaths for 2 files in the dataset:
`log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json`
- Sample of single log file (2018-11-12-events.json):
![alt text](https://video.udacity-data.com/topher/2019/February/5c6c3f0a_log-data/log-data.png)

### Schema
Chosen schema: Star schema optimised for queries on song pllay analysis. The star schema is optimised for Online Analytical Processing (OLAP) operations, which is what the analysts would want to be able to do to find the insights they need.

#### Fact Table
* **songplays** - records in log data associated with song plays 
  * Records with page `NextSong`
  * Columns: `songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent`
  * Primary key: `songplay_id (Identity)` 
  * Unique + NOT NULL constraint: `start_time, user_id`
  * Disk partition: `year, month`

#### Dimension Tables
* **users** - users in the app
  * Columns: `user_id, first_name, last_name, gender, level`
  * Primary key: `user_id`
  * Disk partition: None
* **songs** - songs in music database
  * Columns: `song_id, title, artist_id, year, duration`
  * Primary key: `song_id`
  * NOT NULL constraint: `artist_id`
  * Disk partition: `year, artist_id`
* **artists** - artists in music database
  * Columns: `artist_id, name, location, latitude, longitude`
  * Primary key: `artist_id`
  * Disk partition: None
* **time** - timestamps of records in songplays broken down into specific units
  * Columns: `start_time, hour, day, week, month, year, weekday`
  * Primary key: `start_time`
  * Disk partition: `year, month`

### ETL Pipeline

#### Data Transformation
Data extracted needs to be transformed to fit the data model in the final tables. Here, the main variable that needs to be transformed is simply the time data that would go into the **time** dimension table and the **songplays** fact table. The source data for the timestamp is in unix format and will need to be converted to timestamp - from which, the year, month, day and hour values etc. can be easily extracted for the destination tables.

#### Duplicates
Duplicates would also have to be removed to ensure that they are not part of the final data loaded into the tables.

### Project Files
* **etl.py** reads data from S3, processes that data using Spark, and writes them back to S3.
* **dl.cfg** contains AWS credentials - this will be blank here for security reasons. Do fill in with your own credentials.
* **workspace_utils.py** contains code to delay the Udacity workspace from falling asleep when running code which takes a long time. If not desirable to have this (e.g. you are not running within the Udacity workspace), remove the lines `from workspace_utils import active_session` and `with active_session():` and unindent all code by one tab in **etl.py**.

### Getting Started

#### Prerequisites
* Python 3
* pyspark
* AWS account with the relevant IAM roles and permissions - able to read and write into S3.
* Update AWS access and secret key in `dl.cfg` (configuration file)

`[KEYS]
AWS_ACCESS_KEY_ID= <your AWS access key id>
AWS_SECRET_ACCESS_KEY= <your AWS secret access key>`

#### Re-running the project
* Set all path and folder variables in the `main` method in **etl.py**.
* Run the following code in the terminal: `python etl.py`.

